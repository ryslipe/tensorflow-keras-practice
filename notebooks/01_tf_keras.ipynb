{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training and testing data\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, each image is listed as a 28x28 array. The pixel intensities are represesnted as integers from 0 to 255 and are not floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a validation set from the full training set. It will be the last thousand images listed in the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 28, 28)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class names \n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first class of y_train\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coat\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(class_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first layer that we build. It is a flatten layer and the role is to convert each input image to a 1D array which means it will compute X.reshape(-1, 1). Remember that .reshape(-1, 1) reshapes the data where the -1 uses all rows no matter what the number is and reshapes it in to 1 column. This layer is only meant for preprocessing so that we can use the data. That being said, there are no parameters in this layer. We include the input shape to be the shape of the instances and not the shape of the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create layers to the model\n",
    "model.add(keras.layers.Flatten(input_shape = [28, 28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net comes a dense hidden layer with 300 neurons. This hidden layer will use the ReLu activation function. Each Dense layer has its own weight matrix so for this layer there will be 300 seperate weights for each neuron and the inputs (this means 28x28x300 weights). It also has a bias term which is 1 per neuron. When it receives data it will compute sum(xi*wi) + b. This means there are a total of 784x300 = 235,200 number of weights. Then we add the 300 bias terms for a total of 235,500 parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first hidden layer\n",
    "model.add(keras.layers.Dense(300, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second layer is very similar to the previous but we will have the previous layer number of inputs going to this layer multiplied by the number of neurons in this layer so we have 300x100 weights and add 100 more bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second hidden layer with less neurons and same activation function\n",
    "model.add(keras.layers.Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for the Dense output layer. We have 10 neurons in this layer which is one for each class. Since the classes are exclusive, we use the softmax activation function. The softmax activation function outputs can be interpreted as the probability of that instance belonging to each of the 10 different classes and the sum of them all will equal 1. \n",
    "\n",
    "For this last layer we have the previous number of neurons which is 100. Then multiply by the number of neurons in this layer which is 10 for 1000 parameters then add the 10 bias terms for a total of 1010 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have optionally passed a list of layers like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.Sequential([\n",
    "    #keras.layers.Flatten(input_shape = [28, 28]),\n",
    "    #keras.layers.Dense(300, activation = 'relu'),\n",
    "    #keras.layers.Dense(100, activation = 'relu),\n",
    "    #keras.layers.Dense(10, activation = 'softmax')\n",
    "#])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we see **None** in the output shape it just means the batch size can be of any length. There are a ton of parameters here so the model has a lot of room to learn but also runs the risk of overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View the Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x12be278dcc0>,\n",
       " <keras.layers.core.dense.Dense at 0x12be260fd60>,\n",
       " <keras.layers.core.dense.Dense at 0x12bdbe8cc70>,\n",
       " <keras.layers.core.dense.Dense at 0x12bdbeb5210>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab second layer\n",
    "first_hidden = model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_hidden.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using the get_weights() and set_weights() methods. For a Dense layer it includes the connection weights and bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = first_hidden.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.1036905e-02, -2.5515087e-02,  1.7486721e-02, ...,\n",
       "         5.4033026e-02, -4.8244730e-02,  5.8866292e-03],\n",
       "       [-3.8826175e-02,  4.1814350e-02,  6.1310202e-02, ...,\n",
       "        -4.2923372e-02, -5.8846287e-02,  5.2196756e-02],\n",
       "       [ 4.7915637e-02,  6.3644513e-02, -1.0341309e-02, ...,\n",
       "         5.7841718e-02,  2.4270564e-03, -3.5829384e-02],\n",
       "       ...,\n",
       "       [ 3.3019513e-02,  7.1091548e-02, -2.7268067e-02, ...,\n",
       "        -3.6381185e-05, -7.1757548e-02, -1.4402252e-02],\n",
       "       [ 6.5038159e-02, -7.1910024e-02, -9.1152191e-03, ...,\n",
       "        -9.5477924e-03, -4.1365027e-03,  5.7264954e-02],\n",
       "       [-4.3184578e-02, -6.8999790e-02, -3.4896728e-02, ...,\n",
       "         3.1696320e-02, -5.6283239e-02,  6.9779232e-02]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this layer tells us that there were 784 inputs and they each have 300 weights associated with them. This is because each neuron has a separate weight associated with each of the inputs going to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there should be 784x300 weights\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are normally set to 0 but we can change this using **kernel_initializer** or **bias_initializer** when creating the layer. Kernel is another word for the matrix of connection weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is created, call **compile()** to specify the loss function and optimizer to use. We can also specify a list of other metrics to compute during training and evaluation. Below we use the sparse categorical cross entropy as the loss function, stochastic gradient descent as the optimizer, and accuracy for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the sparse categorical cross entropy loss since we have sparse labels meaning for each instance there is just a target class index from 0 to 9 and we did not use One Hot Encoding and the classes are exclusive. If we did have just one target probability per class for each instance using One Hot Encoded vectors ([0, 0, 0, 1, 0, 0, 0, 0, 0, 0] to represent class 3), we would use categorical_crossentropy instead. If we were doing binary classification we would use the sigmoid activation function in the ouput layer and use the binary_crossentropy loss function instead. \n",
    "\n",
    "If we want to convert sparse labels to one hot labels we can use either keras.utils.to_categorical() function or use np.argmax() with axis = 1. \n",
    "\n",
    "An important note here is that using optimizer='sgd' the learning rate defaults to lr = 0.01 but we normally want to tunr the learning rate so we can instead use optimizer = keras.optimizers.SGD(lr = ???). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7136 - accuracy: 0.7625 - val_loss: 0.4991 - val_accuracy: 0.8346\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4838 - accuracy: 0.8323 - val_loss: 0.4564 - val_accuracy: 0.8458\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4406 - accuracy: 0.8458 - val_loss: 0.4212 - val_accuracy: 0.8572\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4147 - accuracy: 0.8546 - val_loss: 0.3941 - val_accuracy: 0.8676\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3926 - accuracy: 0.8615 - val_loss: 0.3789 - val_accuracy: 0.8724\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3784 - accuracy: 0.8659 - val_loss: 0.3747 - val_accuracy: 0.8660\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3648 - accuracy: 0.8709 - val_loss: 0.3554 - val_accuracy: 0.8754\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3529 - accuracy: 0.8755 - val_loss: 0.3840 - val_accuracy: 0.8606\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3424 - accuracy: 0.8783 - val_loss: 0.3680 - val_accuracy: 0.8660\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3339 - accuracy: 0.8814 - val_loss: 0.3457 - val_accuracy: 0.8746\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3254 - accuracy: 0.8844 - val_loss: 0.3345 - val_accuracy: 0.8808\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3170 - accuracy: 0.8859 - val_loss: 0.3454 - val_accuracy: 0.8802\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3104 - accuracy: 0.8884 - val_loss: 0.3299 - val_accuracy: 0.8850\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3030 - accuracy: 0.8919 - val_loss: 0.3279 - val_accuracy: 0.8848\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2971 - accuracy: 0.8933 - val_loss: 0.3271 - val_accuracy: 0.8830\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2908 - accuracy: 0.8950 - val_loss: 0.3252 - val_accuracy: 0.8838\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2848 - accuracy: 0.8980 - val_loss: 0.3231 - val_accuracy: 0.8816\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2799 - accuracy: 0.8997 - val_loss: 0.3092 - val_accuracy: 0.8888\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2749 - accuracy: 0.9003 - val_loss: 0.3106 - val_accuracy: 0.8906\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2698 - accuracy: 0.9034 - val_loss: 0.3188 - val_accuracy: 0.8842\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2651 - accuracy: 0.9042 - val_loss: 0.3102 - val_accuracy: 0.8892\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2610 - accuracy: 0.9054 - val_loss: 0.3563 - val_accuracy: 0.8674\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2552 - accuracy: 0.9080 - val_loss: 0.3111 - val_accuracy: 0.8900\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2516 - accuracy: 0.9098 - val_loss: 0.3140 - val_accuracy: 0.8892\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2478 - accuracy: 0.9106 - val_loss: 0.3153 - val_accuracy: 0.8868\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2428 - accuracy: 0.9124 - val_loss: 0.3091 - val_accuracy: 0.8922\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2381 - accuracy: 0.9138 - val_loss: 0.3112 - val_accuracy: 0.8924\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2362 - accuracy: 0.9155 - val_loss: 0.3152 - val_accuracy: 0.8862\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2310 - accuracy: 0.9163 - val_loss: 0.3000 - val_accuracy: 0.8906\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2272 - accuracy: 0.9189 - val_loss: 0.3245 - val_accuracy: 0.8856\n"
     ]
    }
   ],
   "source": [
    "# fit the model \n",
    "history = model.fit(X_train, y_train, epochs = 30, \n",
    "                    validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 962us/step - loss: 2.3346 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3346221446990967, 0.10000000149011612]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
